{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##CHI Notebook for Movie Reviews and Elections Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import preprocessor as p\n",
    "from nltk import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import string\n",
    "import pandas as pd\n",
    "from stw import SupervisedTermWeightingWTransformer\n",
    "from numpy import array\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "import simplejson\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###To find out frequency of documents that contain a particular term in the vocabulary###\n",
    "def document_frequency(pos_documents,neg_documents,vocabulary_list):\n",
    "\tpos_documents_freq=[]\n",
    "\tneg_documents_freq=[]\n",
    "\tfor word in vocabulary_list:\n",
    "\t\tpos_count=0\n",
    "\t\tneg_count=0\n",
    "\t\tfor document in pos_documents:\n",
    "\t\t\tif word in document:\n",
    "\t\t\t\tpos_count+=1\n",
    "\t\tpos_documents_freq.append(pos_count)\n",
    "\n",
    "\t\tfor document in neg_documents:\n",
    "\t\t\tif word in document:\n",
    "\t\t\t\tneg_count+=1\n",
    "\t\tneg_documents_freq.append(neg_count)\n",
    "\n",
    "\treturn pos_documents_freq,neg_documents_freq\n",
    "\n",
    "###CHI for Positive Corpus###\n",
    "def CHI_for_positive_corpus(pos_documents,neg_documents,pos_documents_freq,neg_documents_freq):\n",
    "\tpos_D=len(pos_documents)\n",
    "\tneg_D=len(neg_documents)\n",
    "\tD=pos_D+neg_D\n",
    "\tCHI_pos=[]\n",
    "\tfor i in range(len(pos_documents_freq)):\n",
    "\t\tncti=pos_documents_freq[i]\n",
    "\t\tncbtib=neg_D - neg_documents_freq[i]\n",
    "\t\tncbti=neg_documents_freq[i]\n",
    "\t\tnctib=pos_D - pos_documents_freq[i]\n",
    "\n",
    "\n",
    "\t\tnumerator = D * math.pow((ncti * ncbtib - ncbti * nctib ),2)\n",
    "\t\tdenominator=(ncti+nctib)*(ncbti+ncbtib)*(ncti+ncbti)*(nctib+ncbtib)\n",
    "\n",
    "\t\tif denominator == 0:\n",
    "\t\t\tCHI_per_term=0\n",
    "\t\telse:\n",
    "\t\t\tCHI_per_term = float(numerator)/denominator\n",
    "\n",
    "\t\tCHI_pos.append(CHI_per_term)\n",
    "\treturn CHI_pos\n",
    "\n",
    "###CHI for Negative Corpus###\n",
    "def CHI_for_negative_corpus(pos_documents,neg_documents,pos_documents_freq,neg_documents_freq):\n",
    "\tpos_D=len(pos_documents)\n",
    "\tneg_D=len(neg_documents)\n",
    "\tD=pos_D+neg_D\n",
    "\tCHI_neg=[]\n",
    "\tfor i in range(len(neg_documents_freq)):\n",
    "\t\tncti=neg_documents_freq[i]\n",
    "\t\tncbtib=pos_D - pos_documents_freq[i]\n",
    "\t\tncbti=pos_documents_freq[i]\n",
    "\t\tnctib=neg_D - neg_documents_freq[i]\n",
    "\n",
    "\t\tnumerator =  D * math.pow((ncti * ncbtib - ncbti * nctib ),2)\n",
    "\t\tdenominator=(ncti+nctib)*(ncbti+ncbtib)*(ncti+ncbti)*(nctib+ncbtib)\n",
    "\n",
    "\t\tif denominator == 0:\n",
    "\t\t\tCHI_per_term = 0\n",
    "\t\telse:\n",
    "\t\t\tCHI_per_term = float(numerator)/denominator\n",
    "\n",
    "\t\tCHI_neg.append(CHI_per_term)\n",
    "\treturn CHI_neg\n",
    "\n",
    "###Calculating CHI###\n",
    "def CHI(CHI_pos,CHI_neg):\n",
    "\tCHI_result=[]\n",
    "\tfor i in range(len(CHI_pos)):\n",
    "\t\tCHI_result.append(max(CHI_pos[i],CHI_neg[i]))\n",
    "\n",
    "\treturn CHI_result\n",
    "\n",
    "def CHI_mapper(CHI_result,vocabulary_list):\n",
    "    d={}\n",
    "    for i in range(len(vocabulary_list)):\n",
    "        d[vocabulary_list[i]]=CHI_result[i]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elections Vocabulary\n",
    "def make_Corpus(root_dir,polarity_dirs):\n",
    "    corpus = []\n",
    "    for polarity_dir in polarity_dirs:\n",
    "        reviews = [os.path.join(polarity_dir,f) for f in os.listdir(polarity_dir)]\n",
    "        for review in reviews:\n",
    "            doc_string = \"\";\n",
    "            with open(review) as rev:\n",
    "                for line in rev:\n",
    "                    #line = preprocessing(line)\n",
    "                    doc_string = doc_string + line\n",
    "                    doc_string+=\" \"\n",
    "            if not corpus:\n",
    "                corpus = [doc_string]\n",
    "            else:\n",
    "                corpus.append(doc_string)\n",
    "    return corpus\n",
    "\n",
    "root_dir = 'Elections/pos/'\n",
    "pos_corpus = make_Corpus(root_dir,['Elections/pos/'])\n",
    "print(\"Positive Corpus Successful\")\n",
    "\n",
    "root_dir = 'Elections/neg/'\n",
    "neg_corpus = make_Corpus(root_dir,['Elections/neg/'])\n",
    "print(\"Negative Corpus Successful\")\n",
    "\n",
    "corpus=pos_corpus+neg_corpus\n",
    "for i in range(len(corpus)):\n",
    "        corpus[i] = corpus[i].split(\" \")\n",
    "        \n",
    "for i in range(len(pos_corpus)):\n",
    "        pos_corpus[i] = pos_corpus[i].split(\" \")\n",
    "\n",
    "for i in range(len(neg_corpus)):\n",
    "        neg_corpus[i] = neg_corpus[i].split(\" \")\n",
    "\n",
    "\n",
    "def create_vocabulary(corpus):\n",
    "    vocabulary=Counter()\n",
    "    for i in range(len(corpus)):\n",
    "        vocabulary.update(corpus[i])   \n",
    "    vocabulary_list = [word for word,frequency in vocabulary.items() if frequency >= 5]\n",
    "    print(\"Vocabulary Generated\")\n",
    "    \n",
    "    return vocabulary_list\n",
    "\n",
    "election_vocabulary_list=create_vocabulary(corpus)\n",
    "\n",
    "\n",
    "\n",
    "#Election Corpus Results\n",
    "pos_documents_freq,neg_documents_freq=document_frequency(pos_corpus,neg_corpus,election_vocabulary_list)\n",
    "CHI_pos=CHI_for_positive_corpus(pos_corpus,neg_corpus,pos_documents_freq,neg_documents_freq)\n",
    "CHI_neg=CHI_for_negative_corpus(pos_corpus,neg_corpus,pos_documents_freq,neg_documents_freq)\n",
    "CHI_result=CHI(CHI_pos,CHI_neg)\n",
    "d = CHI_mapper(CHI_result,election_vocabulary_list)\n",
    "\n",
    "\n",
    "labels = np.zeros(4472);\n",
    "labels[0:2236]=1;\n",
    "labels[2236:]=0; \n",
    "       \n",
    "kf = StratifiedKFold(n_splits=10)\n",
    " \n",
    "totalsvm = 0           # Accuracy measure on 2000 files\n",
    "totalNB = 0\n",
    "totalLR = 0\n",
    "totalMatSvm = np.zeros((2,2));  # Confusion matrix on 2000 files\n",
    "totalMatNB = np.zeros((2,2));\n",
    "totalMatLR = np.zeros((2,2));\n",
    "\n",
    "\n",
    "\n",
    "#Movie Corpus Results\n",
    "for train_index, test_index in kf.split(corpus,labels):\n",
    "    X_train = [corpus[i] for i in train_index]\n",
    "    X_test = [corpus[i] for i in test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "\n",
    "\n",
    "    CHI_train=[]\n",
    "    for i in range(len(X_train)):\n",
    "        score=[]\n",
    "        for j in range(len(election_vocabulary_list)):\n",
    "            if election_vocabulary_list[j] in X_train[i]:\n",
    "                score.append(d[election_vocabulary_list[j]])\n",
    "            else:\n",
    "                score.append(0.0)\n",
    "        CHI_train.append(score)\n",
    "        \n",
    "    print(\"CHI Training done\")\n",
    "\n",
    "\n",
    "    CHI_test=[]\n",
    "    for i in range(len(X_test)):\n",
    "        score=[]\n",
    "        for j in range(len(election_vocabulary_list)):\n",
    "            if election_vocabulary_list[j] in X_test[i]:\n",
    "                score.append(d[election_vocabulary_list[j]])\n",
    "            else:\n",
    "                score.append(0.0)\n",
    "        CHI_test.append(score)\n",
    "    \n",
    "    print(\"CHI Testing done\")\n",
    "    \n",
    "    model1 = LinearSVC()\n",
    "    model2 = MultinomialNB()   \n",
    "    model3 = LogisticRegression()\n",
    "    model1.fit(CHI_train,y_train)\n",
    "    model2.fit(CHI_train,y_train)\n",
    "    model3.fit(CHI_train,y_train)\n",
    "    result1 = model1.predict(CHI_test)\n",
    "    result2 = model2.predict(CHI_test)\n",
    "    result3 = model3.predict(CHI_test)\n",
    "    \n",
    "     \n",
    "    totalMatSvm = totalMatSvm + confusion_matrix(y_test, result1)\n",
    "    totalMatNB = totalMatNB + confusion_matrix(y_test, result2)\n",
    "    totalMatLR = totalMatLR + confusion_matrix(y_test, result3)\n",
    "    totalsvm = totalsvm+sum(y_test==result1)\n",
    "    totalNB = totalNB+sum(y_test==result2)\n",
    "    totalLR = totalLR+sum(y_test==result3)\n",
    "\n",
    "print(\"########Results########\")\n",
    "print(\"SVM: \",totalMatSvm, totalsvm/4472.0)\n",
    "print(\"NB: \",totalMatNB, totalNB/4472.0)\n",
    "print(\"LR: \",totalMatLR, totalLR/4472.0)\n",
    "print()\n",
    "print()\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"SVM\",f1_score(y_test, result1, average='binary')) \n",
    "print(\"NB\",f1_score(y_test, result2, average='binary')) \n",
    "print(\"LR\",f1_score(y_test, result3, average='binary')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Movie\n",
    "def preprocessing(line):\n",
    "    line=p.clean(line)\n",
    "    line = line.lower()\n",
    "    line = line.split()\n",
    "    for i in range(len(line)):\n",
    "        lemmatizing_token=lemmatizer.lemmatize(line[i])\n",
    "        line[i]=lemmatizing_token\n",
    "    translation = str.maketrans(\"\",\"\", string.punctuation);\n",
    "    for i in range(len(line)):\n",
    "        line[i]=line[i].translate(translation)\n",
    "\n",
    "    line=[token for token in line if token.isalpha()]\n",
    "    line=[token for token in line if len(token)>2]\n",
    "    line = \" \".join(line)\n",
    "    return line\n",
    "\n",
    "def make_Corpus(root_dir,polarity_dirs):\n",
    "    corpus = []\n",
    "    for polarity_dir in polarity_dirs:\n",
    "        reviews = [os.path.join(polarity_dir,f) for f in os.listdir(polarity_dir)]\n",
    "        for review in reviews:\n",
    "            doc_string = \"\";\n",
    "            with open(review) as rev:\n",
    "                for line in rev:\n",
    "                    line = preprocessing(line)\n",
    "                    doc_string = doc_string + line\n",
    "                    doc_string+=\" \"\n",
    "            if not corpus:\n",
    "                corpus = [doc_string]\n",
    "            else:\n",
    "                corpus.append(doc_string)\n",
    "    return corpus\n",
    "\n",
    "root_dir = 'txt_sentoken/neg/'\n",
    "neg_corpus = make_Corpus(root_dir,['txt_sentoken/neg/'])\n",
    "print(\"Successful Negative Corpus\")\n",
    "\n",
    "root_dir = 'txt_sentoken/pos/'\n",
    "pos_corpus = make_Corpus(root_dir,['txt_sentoken/pos/'])\n",
    "print(\"Successful Positive Corpus\")\n",
    "\n",
    "\n",
    "corpus=pos_corpus+neg_corpus\n",
    "for i in range(len(corpus)):\n",
    "        corpus[i] = corpus[i].split(\" \")\n",
    "        \n",
    "for i in range(len(pos_corpus)):\n",
    "        pos_corpus[i] = pos_corpus[i].split(\" \")\n",
    "\n",
    "for i in range(len(neg_corpus)):\n",
    "        neg_corpus[i] = neg_corpus[i].split(\" \")\n",
    "\n",
    "\n",
    "\n",
    "def create_vocabulary(corpus):\n",
    "    vocabulary=Counter()\n",
    "    for i in range(len(corpus)):\n",
    "        vocabulary.update(corpus[i])   \n",
    "    vocabulary_list = [word for word,frequency in vocabulary.items() if frequency >= 5]\n",
    "    print(\"Vocabulary Generated\")\n",
    "    \n",
    "    return vocabulary_list\n",
    "\n",
    "vocabulary_list=create_vocabulary(corpus)\n",
    "\n",
    "labels = np.zeros(2000);\n",
    "labels[0:1000]=1;\n",
    "labels[1000:2000]=0; \n",
    "       \n",
    "kf = StratifiedKFold(n_splits=10)\n",
    " \n",
    "totalsvm = 0           # Accuracy measure on 2000 files\n",
    "totalNB = 0\n",
    "totalLR = 0\n",
    "totalMatSvm = np.zeros((2,2));  # Confusion matrix on 2000 files\n",
    "totalMatNB = np.zeros((2,2));\n",
    "totalMatLR = np.zeros((2,2));\n",
    "\n",
    "\n",
    "pos_documents_freq,neg_documents_freq=document_frequency(pos_corpus,neg_corpus,vocabulary_list)\n",
    "CHI_pos=CHI_for_positive_corpus(pos_corpus,neg_corpus,pos_documents_freq,neg_documents_freq)\n",
    "CHI_neg=CHI_for_negative_corpus(pos_corpus,neg_corpus,pos_documents_freq,neg_documents_freq)\n",
    "CHI_result=CHI(CHI_pos,CHI_neg)\n",
    "d = CHI_mapper(CHI_result,vocabulary_list)\n",
    "\n",
    "#Movie Corpus Results\n",
    "for train_index, test_index in kf.split(corpus,labels):\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    X_train = [corpus[i] for i in train_index]\n",
    "    X_test = [corpus[i] for i in test_index]\n",
    "        \n",
    "    CHI_train=[]\n",
    "    for i in range(len(X_train)):\n",
    "        score=[]\n",
    "        for j in range(len(vocabulary_list)):\n",
    "            if vocabulary_list[j] in X_train[i]:\n",
    "                score.append(d[vocabulary_list[j]])\n",
    "            else:\n",
    "                score.append(0.0)\n",
    "        CHI_train.append(score)\n",
    "    print(\"CHI_train done\")\n",
    "    \n",
    "    CHI_test=[]\n",
    "    for i in range(len(X_test)):\n",
    "        score=[]\n",
    "        for j in range(len(vocabulary_list)):\n",
    "            if vocabulary_list[j] in X_test[i]:\n",
    "                score.append(d[vocabulary_list[j]])\n",
    "            else:\n",
    "                score.append(0.0)\n",
    "        CHI_test.append(score)\n",
    "    \n",
    "    print(\"CHI_test done\")\n",
    "\n",
    "    model1 = LinearSVC()\n",
    "    model2 = MultinomialNB()   \n",
    "    model3 = LogisticRegression()\n",
    "    model1.fit(CHI_train,y_train)\n",
    "    model2.fit(CHI_train,y_train)\n",
    "    model3.fit(CHI_train,y_train)\n",
    "    result1 = model1.predict(CHI_test)\n",
    "    result2 = model2.predict(CHI_test)\n",
    "    result3 = model3.predict(CHI_test)\n",
    "    \n",
    "     \n",
    "    totalMatSvm = totalMatSvm + confusion_matrix(y_test, result1)\n",
    "    totalMatNB = totalMatNB + confusion_matrix(y_test, result2)\n",
    "    totalMatLR = totalMatLR + confusion_matrix(y_test, result3)\n",
    "    totalsvm = totalsvm+sum(y_test==result1)\n",
    "    totalNB = totalNB+sum(y_test==result2)\n",
    "    totalLR = totalLR+sum(y_test==result3)\n",
    "\n",
    "print(\"########Results########\")\n",
    "print(\"SVM: \",totalMatSvm, totalsvm/2000.0)\n",
    "print(\"NB: \",totalMatNB, totalNB/2000.0)\n",
    "print(\"LR: \",totalMatLR, totalLR/2000.0)\n",
    "print()\n",
    "print()\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"SVM\",f1_score(y_test, result1, average='binary')) \n",
    "print(\"NB\",f1_score(y_test, result2, average='binary')) \n",
    "print(\"LR\",f1_score(y_test, result3, average='binary')) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
