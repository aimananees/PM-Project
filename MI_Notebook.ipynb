{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##MI Notebook for Movie Reviews and Elections Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import preprocessor as p\n",
    "from nltk import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import string\n",
    "import pandas as pd\n",
    "from stw import SupervisedTermWeightingWTransformer\n",
    "from numpy import array\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "import simplejson\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###To find out frequency of documents that contain a particular term in the vocabulary###\n",
    "def document_frequency(pos_documents,neg_documents,vocabulary_list):\n",
    "\tpos_documents_freq=[]\n",
    "\tneg_documents_freq=[]\n",
    "\tfor word in vocabulary_list:\n",
    "\t\tpos_count=0\n",
    "\t\tneg_count=0\n",
    "\t\tfor document in pos_documents:\n",
    "\t\t\tif word in document:\n",
    "\t\t\t\tpos_count+=1\n",
    "\t\tpos_documents_freq.append(pos_count)\n",
    "\n",
    "\t\tfor document in neg_documents:\n",
    "\t\t\tif word in document:\n",
    "\t\t\t\tneg_count+=1\n",
    "\t\tneg_documents_freq.append(neg_count)\n",
    "\n",
    "\treturn pos_documents_freq,neg_documents_freq\n",
    "\n",
    "###MI for Positive Corpus###\n",
    "def MI_for_positive_corpus(pos_documents,neg_documents,pos_documents_freq,neg_documents_freq):\n",
    "\tpos_D=len(pos_documents)\n",
    "\tneg_D=len(neg_documents)\n",
    "\tD=pos_D+neg_D\n",
    "\tMI_pos=[]\n",
    "\tfor i in range(len(pos_documents_freq)):\n",
    "\t\tnumerator=pos_documents_freq[i] * D\n",
    "\t\tdenominator=(pos_documents_freq[i]+neg_documents_freq[i])*len(pos_documents)\n",
    "\n",
    "\t\tif denominator == 0 or float(numerator)/denominator == 0:\n",
    "\t\t\tMI_pos.append(0)\n",
    "\t\telse:\n",
    "\t\t\tMI_per_term = float(numerator)/denominator\n",
    "\t\t\tMI_per_term=math.log(MI_per_term,2)\n",
    "\t\t\tMI_pos.append(MI_per_term)\n",
    "\n",
    "\treturn MI_pos\n",
    "\n",
    "###MI for Negative Corpus###\n",
    "def MI_for_negative_corpus(pos_documents,neg_documents,pos_documents_freq,neg_documents_freq):\n",
    "\tpos_D=len(pos_documents)\n",
    "\tneg_D=len(neg_documents)\n",
    "\tD=pos_D+neg_D\n",
    "\tMI_neg=[]\n",
    "\tfor i in range(len(neg_documents_freq)):\n",
    "\t\tnumerator=neg_documents_freq[i] * D\n",
    "\t\tdenominator=(pos_documents_freq[i]+neg_documents_freq[i])*len(neg_documents)\n",
    "\n",
    "\t\tif denominator == 0 or float(numerator)/denominator == 0:\n",
    "\t\t\tMI_neg.append(0)\n",
    "\t\telse:\n",
    "\t\t\tMI_per_term = float(numerator)/denominator\n",
    "\t\t\tMI_per_term=math.log(MI_per_term,2)\n",
    "\t\t\tMI_neg.append(MI_per_term)\n",
    "\n",
    "\treturn MI_neg\n",
    "\n",
    "###Calculating MI###\n",
    "def MI(MI_pos,MI_neg):\n",
    "\tMI_result=[]\n",
    "\tfor i in range(len(MI_pos)):\n",
    "\t\tMI_result.append(max(MI_pos[i],MI_neg[i]))\n",
    "\n",
    "\treturn MI_result\n",
    "\n",
    "def MI_mapper(MI_result,vocabulary_list):\n",
    "    d={}\n",
    "    for i in range(len(vocabulary_list)):\n",
    "        d[vocabulary_list[i]]=MI_result[i]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elections Vocabulary\n",
    "def make_Corpus(root_dir,polarity_dirs):\n",
    "    corpus = []\n",
    "    for polarity_dir in polarity_dirs:\n",
    "        reviews = [os.path.join(polarity_dir,f) for f in os.listdir(polarity_dir)]\n",
    "        for review in reviews:\n",
    "            doc_string = \"\";\n",
    "            with open(review) as rev:\n",
    "                for line in rev:\n",
    "                    #line = preprocessing(line)\n",
    "                    doc_string = doc_string + line\n",
    "                    doc_string+=\" \"\n",
    "            if not corpus:\n",
    "                corpus = [doc_string]\n",
    "            else:\n",
    "                corpus.append(doc_string)\n",
    "    return corpus\n",
    "\n",
    "root_dir = 'Elections/pos/'\n",
    "pos_corpus = make_Corpus(root_dir,['Elections/pos/'])\n",
    "print(\"Positive Corpus Successful\")\n",
    "\n",
    "root_dir = 'Elections/neg/'\n",
    "neg_corpus = make_Corpus(root_dir,['Elections/neg/'])\n",
    "print(\"Negative Corpus Successful\")\n",
    "\n",
    "corpus=pos_corpus+neg_corpus\n",
    "for i in range(len(corpus)):\n",
    "        corpus[i] = corpus[i].split(\" \")\n",
    "        \n",
    "for i in range(len(pos_corpus)):\n",
    "        pos_corpus[i] = pos_corpus[i].split(\" \")\n",
    "\n",
    "for i in range(len(neg_corpus)):\n",
    "        neg_corpus[i] = neg_corpus[i].split(\" \")\n",
    "\n",
    "\n",
    "def create_vocabulary(corpus):\n",
    "    vocabulary=Counter()\n",
    "    for i in range(len(corpus)):\n",
    "        vocabulary.update(corpus[i])   \n",
    "    vocabulary_list = [word for word,frequency in vocabulary.items() if frequency >= 5]\n",
    "    print(\"Vocabulary Generated\")\n",
    "    \n",
    "    return vocabulary_list\n",
    "\n",
    "election_vocabulary_list=create_vocabulary(corpus)\n",
    "\n",
    "\n",
    "\n",
    "#Election Corpus Results\n",
    "pos_documents_freq,neg_documents_freq=document_frequency(pos_corpus,neg_corpus,election_vocabulary_list)\n",
    "MI_pos=MI_for_positive_corpus(pos_corpus,neg_corpus,pos_documents_freq,neg_documents_freq)\n",
    "MI_neg=MI_for_negative_corpus(pos_corpus,neg_corpus,pos_documents_freq,neg_documents_freq)\n",
    "MI_result=MI(MI_pos,MI_neg)\n",
    "d = MI_mapper(MI_result,election_vocabulary_list)\n",
    "\n",
    "labels = np.zeros(4472);\n",
    "labels[0:2236]=1;\n",
    "labels[2236:]=0; \n",
    "       \n",
    "kf = StratifiedKFold(n_splits=10)\n",
    " \n",
    "totalsvm = 0           # Accuracy measure on 2000 files\n",
    "totalNB = 0\n",
    "totalLR = 0\n",
    "totalMatSvm = np.zeros((2,2));  # Confusion matrix on 2000 files\n",
    "totalMatNB = np.zeros((2,2));\n",
    "totalMatLR = np.zeros((2,2));\n",
    "\n",
    "for train_index, test_index in kf.split(corpus,labels):\n",
    "    X_train = [corpus[i] for i in train_index]\n",
    "    X_test = [corpus[i] for i in test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    MI_train=[]\n",
    "    for i in range(len(X_train)):\n",
    "        score=[]\n",
    "        for j in range(len(election_vocabulary_list)):\n",
    "            if election_vocabulary_list[j] in X_train[i]:\n",
    "                score.append(d[election_vocabulary_list[j]])\n",
    "            else:\n",
    "                score.append(0.0)\n",
    "        MI_train.append(score)\n",
    "    \n",
    "    print(\"MI Train Done\")\n",
    "        \n",
    "    \n",
    "    MI_test=[]\n",
    "    for i in range(len(X_test)):\n",
    "        score=[]\n",
    "        for j in range(len(election_vocabulary_list)):\n",
    "            if election_vocabulary_list[j] in X_test[i]:\n",
    "                score.append(d[election_vocabulary_list[j]])\n",
    "            else:\n",
    "                score.append(0.0)\n",
    "        MI_test.append(score)\n",
    "    \n",
    "    print(\"MI Test Done\")\n",
    "\n",
    "    \n",
    "    model1 = LinearSVC()\n",
    "    model2 = MultinomialNB()   \n",
    "    model3 = LogisticRegression()\n",
    "    model1.fit(MI_train,y_train)\n",
    "    model2.fit(MI_train,y_train)\n",
    "    model3.fit(MI_train,y_train)\n",
    "    result1 = model1.predict(MI_test)\n",
    "    result2 = model2.predict(MI_test)\n",
    "    result3 = model3.predict(MI_test)\n",
    "    \n",
    "     \n",
    "    totalMatSvm = totalMatSvm + confusion_matrix(y_test, result1)\n",
    "    totalMatNB = totalMatNB + confusion_matrix(y_test, result2)\n",
    "    totalMatLR = totalMatLR + confusion_matrix(y_test, result3)\n",
    "    totalsvm = totalsvm+sum(y_test==result1)\n",
    "    totalNB = totalNB+sum(y_test==result2)\n",
    "    totalLR = totalLR+sum(y_test==result3)\n",
    "\n",
    "print(\"########Results########\")\n",
    "print(\"SVM: \",totalMatSvm, totalsvm/4472.0)\n",
    "print(\"NB: \",totalMatNB, totalNB/4472.0)\n",
    "print(\"LR: \",totalMatLR, totalLR/4472.0)\n",
    "print()\n",
    "print()\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"SVM\",f1_score(y_test, result1, average='binary')) \n",
    "print(\"NB\",f1_score(y_test, result2, average='binary')) \n",
    "print(\"LR\",f1_score(y_test, result3, average='binary')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Movie\n",
    "def preprocessing(line):\n",
    "    line=p.clean(line)\n",
    "    line = line.lower()\n",
    "    line = line.split()\n",
    "    for i in range(len(line)):\n",
    "        lemmatizing_token=lemmatizer.lemmatize(line[i])\n",
    "        line[i]=lemmatizing_token\n",
    "    translation = str.maketrans(\"\",\"\", string.punctuation);\n",
    "    for i in range(len(line)):\n",
    "        line[i]=line[i].translate(translation)\n",
    "\n",
    "    line=[token for token in line if token.isalpha()]\n",
    "    line=[token for token in line if len(token)>2]\n",
    "    line = \" \".join(line)\n",
    "    return line\n",
    "\n",
    "def make_Corpus(root_dir,polarity_dirs):\n",
    "    corpus = []\n",
    "    for polarity_dir in polarity_dirs:\n",
    "        reviews = [os.path.join(polarity_dir,f) for f in os.listdir(polarity_dir)]\n",
    "        for review in reviews:\n",
    "            doc_string = \"\";\n",
    "            with open(review) as rev:\n",
    "                for line in rev:\n",
    "                    line = preprocessing(line)\n",
    "                    doc_string = doc_string + line\n",
    "                    doc_string+=\" \"\n",
    "            if not corpus:\n",
    "                corpus = [doc_string]\n",
    "            else:\n",
    "                corpus.append(doc_string)\n",
    "    return corpus\n",
    "\n",
    "root_dir = 'txt_sentoken/pos/'\n",
    "pos_corpus = make_Corpus(root_dir,['txt_sentoken/pos/'])\n",
    "print(\"Successful Positive Corpus\")\n",
    "\n",
    "root_dir = 'txt_sentoken/neg/'\n",
    "neg_corpus = make_Corpus(root_dir,['txt_sentoken/neg/'])\n",
    "print(\"Successful Negative Corpus\")\n",
    "\n",
    "corpus=pos_corpus+neg_corpus\n",
    "for i in range(len(corpus)):\n",
    "        corpus[i] = corpus[i].split(\" \")\n",
    "        \n",
    "for i in range(len(pos_corpus)):\n",
    "        pos_corpus[i] = pos_corpus[i].split(\" \")\n",
    "\n",
    "for i in range(len(neg_corpus)):\n",
    "        neg_corpus[i] = neg_corpus[i].split(\" \")\n",
    "\n",
    "\n",
    "def create_vocabulary(corpus):\n",
    "    vocabulary=Counter()\n",
    "    for i in range(len(corpus)):\n",
    "        vocabulary.update(corpus[i])   \n",
    "    vocabulary_list = [word for word,frequency in vocabulary.items() if frequency >= 5]\n",
    "    print(\"Vocabulary Generated\")\n",
    "    \n",
    "    return vocabulary_list\n",
    "\n",
    "vocabulary_list=create_vocabulary(corpus)\n",
    "\n",
    "#Movie Corpus Results\n",
    "pos_documents_freq,neg_documents_freq=document_frequency(pos_corpus,neg_corpus,vocabulary_list)\n",
    "MI_pos=MI_for_positive_corpus(pos_corpus,neg_corpus,pos_documents_freq,neg_documents_freq)\n",
    "MI_neg=MI_for_negative_corpus(pos_corpus,neg_corpus,pos_documents_freq,neg_documents_freq)\n",
    "MI_result=MI(MI_pos,MI_neg)\n",
    "d = MI_mapper(MI_result,vocabulary_list)\n",
    "\n",
    "#Movie Corpus Results\n",
    "labels = np.zeros(2000);\n",
    "labels[0:1000]=1;\n",
    "labels[1000:2000]=0; \n",
    "       \n",
    "kf = StratifiedKFold(n_splits=10)\n",
    " \n",
    "totalsvm = 0           # Accuracy measure on 2000 files\n",
    "totalNB = 0\n",
    "totalLR = 0\n",
    "totalMatSvm = np.zeros((2,2));  # Confusion matrix on 2000 files\n",
    "totalMatNB = np.zeros((2,2));\n",
    "totalMatLR = np.zeros((2,2));\n",
    "\n",
    "for train_index, test_index in kf.split(corpus,labels):\n",
    "    X_train = [corpus[i] for i in train_index]\n",
    "    X_test = [corpus[i] for i in test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    MI_train=[]\n",
    "    for i in range(len(X_train)):\n",
    "        score=[]\n",
    "        for j in range(len(vocabulary_list)):\n",
    "            if vocabulary_list[j] in X_train[i]:\n",
    "                score.append(d[vocabulary_list[j]])\n",
    "            else:\n",
    "                score.append(0.0)\n",
    "        MI_train.append(score)\n",
    "    \n",
    "    print(\"MI Training done\")\n",
    "\n",
    "    MI_test=[]\n",
    "    for i in range(len(X_test)):\n",
    "        score=[]\n",
    "        for j in range(len(vocabulary_list)):\n",
    "            if vocabulary_list[j] in X_test[i]:\n",
    "                score.append(d[vocabulary_list[j]])\n",
    "            else:\n",
    "                score.append(0.0)\n",
    "        MI_test.append(score)\n",
    "    \n",
    "    print(\"MI Testing done\")\n",
    "    \n",
    "    model1 = LinearSVC()\n",
    "    model2 = MultinomialNB()   \n",
    "    model3 = LogisticRegression()\n",
    "    model1.fit(MI_train,y_train)\n",
    "    model2.fit(MI_train,y_train)\n",
    "    model3.fit(MI_train,y_train)\n",
    "    result1 = model1.predict(MI_test)\n",
    "    result2 = model2.predict(MI_test)\n",
    "    result3 = model3.predict(MI_test)\n",
    "    \n",
    "     \n",
    "    totalMatSvm = totalMatSvm + confusion_matrix(y_test, result1)\n",
    "    totalMatNB = totalMatNB + confusion_matrix(y_test, result2)\n",
    "    totalMatLR = totalMatLR + confusion_matrix(y_test, result3)\n",
    "    totalsvm = totalsvm+sum(y_test==result1)\n",
    "    totalNB = totalNB+sum(y_test==result2)\n",
    "    totalLR = totalLR+sum(y_test==result3)\n",
    "    \n",
    "\n",
    "print(\"########Results########\")\n",
    "print(\"SVM: \",totalMatSvm, totalsvm/2000.0)\n",
    "print(\"NB: \",totalMatNB, totalNB/2000.0)\n",
    "print(\"LR: \",totalMatLR, totalLR/2000.0)\n",
    "print()\n",
    "print()\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"SVM\",f1_score(y_test, result1, average='binary')) \n",
    "print(\"NB\",f1_score(y_test, result2, average='binary')) \n",
    "print(\"LR\",f1_score(y_test, result3, average='binary')) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
