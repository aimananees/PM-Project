{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##TFIDF Notebook for Movie Reviews and Elections Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import preprocessor as p\n",
    "from nltk import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import string\n",
    "import pandas as pd\n",
    "from stw import SupervisedTermWeightingWTransformer\n",
    "from numpy import array\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "import simplejson\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Term Frequency###\n",
    "def term_frequency(documents,vocabulary_list):\n",
    "\tTF=[]\n",
    "\tfor document in documents:\n",
    "\t\ttf_per_document=[]\n",
    "\t\tfor word in vocabulary_list:\n",
    "\t\t\tfij=document.count(word)\n",
    "\t\t\tif fij>0:\n",
    "\t\t\t\ttf=1+math.log(fij,2)\n",
    "\t\t\telse:\n",
    "\t\t\t\ttf=0\n",
    "\t\t\ttf_per_document.append(tf)\n",
    "\t\tTF.append(tf_per_document)\n",
    "\n",
    "\treturn TF\n",
    "\n",
    "###Term Frequency According to paper###\n",
    "def term_frequency_paper(documents,vocabulary_list):\n",
    "\tTF=[]\n",
    "\n",
    "\tfor document in documents:\n",
    "\t\tN=len(document)\n",
    "\n",
    "\t\ttf_per_document=[]\n",
    "\t\tfor word in vocabulary_list:\n",
    "\t\t\tfij=document.count(word)\n",
    "\n",
    "\t\t\ttf=float(fij)/N\n",
    "\t\t\ttf_per_document.append(tf)\n",
    "\t\tTF.append(tf_per_document)\n",
    "\n",
    "\treturn TF\n",
    "\n",
    "###Inverse Document Frequency###\n",
    "def inverse_document_frequency(documents,vocabulary_list):\n",
    "    IDF=[]\n",
    "    N=len(documents)\n",
    "    for word in vocabulary_list:\n",
    "        count=0\n",
    "        for document in documents:\n",
    "            if word in document:\n",
    "                count+=1\n",
    "        if count == 0:\n",
    "            idf=0\n",
    "        else:\n",
    "            idf=math.log(N/count,2)\n",
    "        IDF.append(idf)\n",
    "    return IDF\n",
    "\n",
    "\n",
    "###Term Frequency Inverse Document Frequency###\n",
    "def term_frequency_inverse_document_frequency(TF,IDF):\n",
    "\tIDF=np.array(IDF)\n",
    "\tTFIDF=[]\n",
    "\tfor tf in TF:\n",
    "\t\ttf=np.array(tf)\n",
    "\t\ttfidf=tf*IDF\n",
    "\t\tTFIDF.append(tfidf.tolist())\n",
    "\treturn TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Elections\n",
    "def preprocessing(line):\n",
    "    line=p.clean(line)\n",
    "    line = line.lower()\n",
    "    line = line.split()\n",
    "    for i in range(len(line)):\n",
    "        lemmatizing_token=lemmatizer.lemmatize(line[i])\n",
    "        line[i]=lemmatizing_token\n",
    "    translation = str.maketrans(\"\",\"\", string.punctuation);\n",
    "    for i in range(len(line)):\n",
    "        line[i]=line[i].translate(translation)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    line=[token for token in line if not token in stop_words]\n",
    "    line=[token for token in line if token.isalpha()]\n",
    "    line=[token for token in line if len(token)>2]\n",
    "    line = \" \".join(line)\n",
    "    return line\n",
    "\n",
    "def make_Corpus(root_dir,polarity_dirs):\n",
    "    corpus = []\n",
    "    for polarity_dir in polarity_dirs:\n",
    "        reviews = [os.path.join(polarity_dir,f) for f in os.listdir(polarity_dir)]\n",
    "        for review in reviews:\n",
    "            doc_string = \"\";\n",
    "            with open(review) as rev:\n",
    "                for line in rev:\n",
    "                    line = preprocessing(line)\n",
    "                    doc_string = doc_string + line\n",
    "                    doc_string+=\" \"\n",
    "            if not corpus:\n",
    "                corpus = [doc_string]\n",
    "            else:\n",
    "                corpus.append(doc_string)\n",
    "    return corpus\n",
    "\n",
    "root_dir = 'Elections/pos/'\n",
    "pos_corpus = make_Corpus(root_dir,['Elections/pos/'])\n",
    "print(\"Successful Positive Corpus\")\n",
    "\n",
    "root_dir = 'Elections/neg/'\n",
    "neg_corpus = make_Corpus(root_dir,['Elections/neg/'])\n",
    "print(\"Successful Negative Corpus\")\n",
    "\n",
    "\n",
    "corpus=pos_corpus+neg_corpus\n",
    "for i in range(len(corpus)):\n",
    "        corpus[i] = corpus[i].split(\" \")\n",
    "\n",
    "def create_vocabulary(corpus):\n",
    "    vocabulary=Counter()\n",
    "    for i in range(len(corpus)):\n",
    "        vocabulary.update(corpus[i])   \n",
    "    vocabulary_list = [word for word,frequency in vocabulary.items() if frequency >= 5]\n",
    "    print(\"Vocabulary Generated\")\n",
    "    \n",
    "    return vocabulary_list\n",
    "\n",
    "#vocabulary_list=create_vocabulary(training_corpus)\n",
    "vocabulary_list=create_vocabulary(corpus)\n",
    "\n",
    "#Movie Corpus Results\n",
    "labels = np.zeros(4472);\n",
    "labels[0:2236]=1;\n",
    "labels[2236:]=0; \n",
    "       \n",
    "kf = StratifiedKFold(n_splits=10)\n",
    " \n",
    "totalsvm = 0           # Accuracy measure on 2000 files\n",
    "totalNB = 0\n",
    "totalLR = 0\n",
    "totalMatSvm = np.zeros((2,2));  # Confusion matrix on 2000 files\n",
    "totalMatNB = np.zeros((2,2));\n",
    "totalMatLR = np.zeros((2,2));\n",
    "\n",
    "for train_index, test_index in kf.split(corpus,labels):\n",
    "    X_train = [corpus[i] for i in train_index]\n",
    "    X_test = [corpus[i] for i in test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    \n",
    "    TF=term_frequency_paper(X_train,vocabulary_list)\n",
    "    IDF=inverse_document_frequency(X_train,vocabulary_list)\n",
    "    train_corpus_tf_idf=term_frequency_inverse_document_frequency(TF,IDF)\n",
    "    print(\"train_corpus_tf_idf done\")\n",
    "\n",
    "    TF=term_frequency_paper(X_test,vocabulary_list)\n",
    "    IDF=inverse_document_frequency(X_test,vocabulary_list)\n",
    "    test_corpus_tf_idf=term_frequency_inverse_document_frequency(TF,IDF)\n",
    "    print(\"test_corpus_tf_idf done\")\n",
    "\n",
    "\n",
    "    \n",
    "    model1 = LinearSVC()\n",
    "    model2 = MultinomialNB()   \n",
    "    model3 = LogisticRegression()\n",
    "    model1.fit(train_corpus_tf_idf,y_train)\n",
    "    model2.fit(train_corpus_tf_idf,y_train)\n",
    "    model3.fit(train_corpus_tf_idf,y_train)\n",
    "    result1 = model1.predict(test_corpus_tf_idf)\n",
    "    result2 = model2.predict(test_corpus_tf_idf)\n",
    "    result3 = model3.predict(test_corpus_tf_idf)\n",
    "    \n",
    "     \n",
    "    totalMatSvm = totalMatSvm + confusion_matrix(y_test, result1)\n",
    "    totalMatNB = totalMatNB + confusion_matrix(y_test, result2)\n",
    "    totalMatLR = totalMatLR + confusion_matrix(y_test, result3)\n",
    "    totalsvm = totalsvm+sum(y_test==result1)\n",
    "    totalNB = totalNB+sum(y_test==result2)\n",
    "    totalLR = totalLR+sum(y_test==result3)\n",
    "\n",
    "print(\"########Results########\")\n",
    "print(\"SVM: \",totalMatSvm, totalsvm/4472.0)\n",
    "print(\"NB: \",totalMatNB, totalNB/4472.0)\n",
    "print(\"LR: \",totalMatLR, totalLR/4472.0)\n",
    "print()\n",
    "print()\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"SVM\",f1_score(y_test, result1, average='binary')) \n",
    "print(\"NB\",f1_score(y_test, result2, average='binary')) \n",
    "print(\"LR\",f1_score(y_test, result3, average='binary')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Movie\n",
    "def preprocessing(line):\n",
    "    line=p.clean(line)\n",
    "    line = line.lower()\n",
    "    line = line.split()\n",
    "    for i in range(len(line)):\n",
    "        lemmatizing_token=lemmatizer.lemmatize(line[i])\n",
    "        line[i]=lemmatizing_token\n",
    "    translation = str.maketrans(\"\",\"\", string.punctuation);\n",
    "    for i in range(len(line)):\n",
    "        line[i]=line[i].translate(translation)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    line=[token for token in line if not token in stop_words]\n",
    "    line=[token for token in line if token.isalpha()]\n",
    "    line=[token for token in line if len(token)>2]\n",
    "    line = \" \".join(line)\n",
    "    return line\n",
    "\n",
    "def make_Corpus(root_dir,polarity_dirs):\n",
    "    corpus = []\n",
    "    for polarity_dir in polarity_dirs:\n",
    "        reviews = [os.path.join(polarity_dir,f) for f in os.listdir(polarity_dir)]\n",
    "        for review in reviews:\n",
    "            doc_string = \"\";\n",
    "            with open(review) as rev:\n",
    "                for line in rev:\n",
    "                    line = preprocessing(line)\n",
    "                    doc_string = doc_string + line\n",
    "                    doc_string+=\" \"\n",
    "            if not corpus:\n",
    "                corpus = [doc_string]\n",
    "            else:\n",
    "                corpus.append(doc_string)\n",
    "    return corpus\n",
    "\n",
    "root_dir = 'txt_sentoken/pos/'\n",
    "pos_corpus = make_Corpus(root_dir,['txt_sentoken/pos/'])\n",
    "print(\"Successful Negative Corpus\")\n",
    "\n",
    "root_dir = 'txt_sentoken/neg/'\n",
    "neg_corpus = make_Corpus(root_dir,['txt_sentoken/neg/'])\n",
    "print(\"Successful Positive Corpus\")\n",
    "\n",
    "\n",
    "corpus=pos_corpus+neg_corpus\n",
    "for i in range(len(corpus)):\n",
    "        corpus[i] = corpus[i].split(\" \")\n",
    "\n",
    "def create_vocabulary(corpus):\n",
    "    vocabulary=Counter()\n",
    "    for i in range(len(corpus)):\n",
    "        vocabulary.update(corpus[i])   \n",
    "    vocabulary_list = [word for word,frequency in vocabulary.items() if frequency >= 5]\n",
    "    print(\"Vocabulary Generated\")\n",
    "    \n",
    "    return vocabulary_list\n",
    "\n",
    "#vocabulary_list=create_vocabulary(training_corpus)\n",
    "vocabulary_list=create_vocabulary(corpus)\n",
    "\n",
    "#Movie Corpus Results\n",
    "labels = np.zeros(2000);\n",
    "labels[0:1000]=1;\n",
    "labels[1000:2000]=0; \n",
    "       \n",
    "kf = StratifiedKFold(n_splits=10)\n",
    " \n",
    "totalsvm = 0           # Accuracy measure on 2000 files\n",
    "totalNB = 0\n",
    "totalLR = 0\n",
    "totalMatSvm = np.zeros((2,2));  # Confusion matrix on 2000 files\n",
    "totalMatNB = np.zeros((2,2));\n",
    "totalMatLR = np.zeros((2,2));\n",
    "\n",
    "for train_index, test_index in kf.split(corpus,labels):\n",
    "    X_train = [corpus[i] for i in train_index]\n",
    "    X_test = [corpus[i] for i in test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    \n",
    "    TF=term_frequency_paper(X_train,vocabulary_list)\n",
    "    IDF=inverse_document_frequency(X_train,vocabulary_list)\n",
    "    train_corpus_tf_idf=term_frequency_inverse_document_frequency(TF,IDF)\n",
    "\n",
    "    TF=term_frequency_paper(X_test,vocabulary_list)\n",
    "    IDF=inverse_document_frequency(X_test,vocabulary_list)\n",
    "    test_corpus_tf_idf=term_frequency_inverse_document_frequency(TF,IDF)\n",
    "\n",
    "\n",
    "    \n",
    "    model1 = LinearSVC()\n",
    "    model2 = MultinomialNB()   \n",
    "    model3 = LogisticRegression()\n",
    "    model1.fit(train_corpus_tf_idf,y_train)\n",
    "    model2.fit(train_corpus_tf_idf,y_train)\n",
    "    model3.fit(train_corpus_tf_idf,y_train)\n",
    "    result1 = model1.predict(test_corpus_tf_idf)\n",
    "    result2 = model2.predict(test_corpus_tf_idf)\n",
    "    result3 = model3.predict(test_corpus_tf_idf)\n",
    "    \n",
    "     \n",
    "    totalMatSvm = totalMatSvm + confusion_matrix(y_test, result1)\n",
    "    totalMatNB = totalMatNB + confusion_matrix(y_test, result2)\n",
    "    totalMatLR = totalMatLR + confusion_matrix(y_test, result3)\n",
    "    totalsvm = totalsvm+sum(y_test==result1)\n",
    "    totalNB = totalNB+sum(y_test==result2)\n",
    "    totalLR = totalLR+sum(y_test==result3)\n",
    "\n",
    "print(\"########Results########\")\n",
    "print(\"SVM: \",totalMatSvm, totalsvm/2000.0)\n",
    "print(\"NB: \",totalMatNB, totalNB/2000.0)\n",
    "print(\"LR: \",totalMatLR, totalLR/2000.0)\n",
    "print()\n",
    "print()\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"SVM\",f1_score(y_test, result1, average='binary')) \n",
    "print(\"NB\",f1_score(y_test, result2, average='binary')) \n",
    "print(\"LR\",f1_score(y_test, result3, average='binary')) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
